# 🧪 Manual QA Testing – AutomationExercise.com

This repository showcases a manual quality assurance (QA) testing project for [AutomationExercise.com](https://www.automationexercise.com/). It includes a detailed test plan, documented test cases, and tracked bugs, simulating a real-world QA workflow using publicly available web content.

### 📌 Objective

To validate the core functionality, usability, and user experience of the AutomationExercise website by executing manual test cases, identifying defects, and documenting results in a structured, professional format. This project highlights key QA competencies for portfolio and professional development purposes.


### 🛠️ Tools Used

- 📊 **Google Sheets** – Test case documentation and execution tracking  
- 🐛 **GitHub Issues** – Bug tracking with steps, severity, and screenshots  
- 🌐 **BrowserStack** – Cross-browser and responsive testing  
- 🖥️ **VS Code** – Markdown editing and documentation  
- 📸 **Snipping Tool** – Screenshot capture for defect documentation  


### 📄 Test Plan

The full test plan outlines the scope, approach, test environment, types of testing, data used, and deliverables.  
→ 📘 [View Test Plan](./test_plan.md)



### ✅ Test Cases

Manual test cases were adapted from the official 26 test scenarios published on the AutomationExercise site.

- **Format**: Google Sheets
- **Status**: Tracked with pass/fail results
- **Fields**: Test Case ID, Title, Steps, Expected Result, Actual Result, Status, Notes

→ 📊 [View Test Cases (Google Sheets)](https://docs.google.com/spreadsheets/d/1Myw9FfFgjnqnkH1LR_Fv-Nxo1xYiNmGZH8jzVdrM9lM/edit?gid=0#gid=0)



### 🐞 Bug Reports

All discovered issues are documented using [GitHub Issues](https://github.com/melatemarkos/qa-ecommerce-manual/issues), following a structured format:

- Title
- UAT Environment
- Test Data (if applicable)
- Steps to Reproduce  
- Expected vs Actual Result  
- Severity Level
- Priority Level
- Screenshots (if applicable)

→ 🐛 [View Reported Issues](https://github.com/melatemarkos/qa-ecommerce-manual/issues)



### 📱 Cross-Browser Testing

Tested on the following browsers/platforms via BrowserStack:

- Google Chrome, Mozilla Firefox, Safari, Microsoft Edge. ![Chrome](https://img.shields.io/badge/Chrome-4285F4?style=flat-square&logo=googlechrome&logoColor=white)
![Firefox](https://img.shields.io/badge/Firefox-FF7139?style=flat-square&logo=firefox-browser&logoColor=white)
![Safari](https://img.shields.io/badge/Safari-000000?style=flat-square&logo=safari&logoColor=white)
![Edge](https://img.shields.io/badge/Edge-0078D7?style=flat-square&logo=microsoft-edge&logoColor=white)


- Windows 10, macOS, iOS, Android (emulated) ![Windows 10](https://img.shields.io/badge/Windows%2010-0078D6?style=flat-square&logo=windows&logoColor=white)
![macOS](https://img.shields.io/badge/macOS-000000?style=flat-square&logo=apple&logoColor=white)
![iOS](https://img.shields.io/badge/iOS-000000?style=flat-square&logo=apple&logoColor=white)
![Android](https://img.shields.io/badge/Android-3DDC84?style=flat-square&logo=android&logoColor=white)

### 📬 Deliverables

- ✔️ Test Plan (`test_plan.md`)
- ✔️ Google Sheet with full test cases
- ✔️ Defect reports via GitHub Issues
- ✔️ Final test summary & insights



### 🙋‍♀️ About This Project

This is a solo QA initiative designed to simulate a real-world manual testing process using only publicly accessible web functionality. No internal access or automation was used. It serves as a practical portfolio project to demonstrate:

- 🧪 Manual testing methodology  
- 📋 Test case execution  
- 🐞 Bug documentation  
- ✍️ Professional QA documentation  





